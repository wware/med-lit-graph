services:
  ollama:
    build:
      context: ./ingestion
      dockerfile: Dockerfile.ollama
    #ports:
    #- "11434:11434"
    environment:
      - LANGUAGE_MODEL=${LANGUAGE_MODEL:-llama3.1:8b}
    volumes:
      # Use a host directory to persist Ollama models
      - ~/ollama_data:/root/.ollama
      # This line is optional. You can use it to mount pre-downloaded models.
      # - ${OLLAMA_MODELS:-./models}:/models

  postgres:
    image: pgvector/pgvector:pg16
    environment:
      - POSTGRES_DB=medgraph
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - "${POSTGRES_DATA_DIR:-./data/postgres}:/var/lib/postgresql/data"
      - ./schema/migration.sql:/docker-entrypoint-initdb.d/init.sql

  ingest:
    build:
      context: .  # Use repo root as build context
      dockerfile: ingestion/Dockerfile
    environment:
      # The ingest script needs to know where the Ollama API is.
      # 'ollama' is the service name in this docker-compose file.
      # Override with: export OLLAMA_HOST=http://<remote-ip>:11434
      - OLLAMA_HOST=${OLLAMA_HOST:-http://ollama:11434}
      - DATABASE_URL=postgresql://postgres:postgres@postgres:5432/medgraph
    depends_on:
      - ollama
      - postgres
    volumes:
      - ./ingestion/outputs:/app/outputs
      - ./ingestion/data/huggingface:/root/.cache/huggingface
    # 'command' is overridden by the command line arguments you provide.
    # This default is just for documentation.
    command: >
      python -m ingestion.ingest_papers
      --query "search terms"
      --limit 10
      --model "llama3.1:70b"


